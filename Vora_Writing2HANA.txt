/*
-- reset
rm /home/vora/hanadata.csv
hdfs dfs -rm /user/vora/hanadata.csv
rm /home/vora/extravoradata.csv
hdfs dfs -rm /user/vora/extravoradata.csv
rm /home/vora/evenmorevoradata.csv
hdfs dfs -rm /user/vora/evenmorevoradata.csv
hdfs dfs -ls /user/vora
ls -l /home/vora

-- sqlContext.sql(s"""drop table HANA_DATA""")
*/

sudo su -
su - vora
sh start-spark-shell.sh
CRTL-L

============== 
== IN SCALA ==
============== 

import org.apache.spark.sql._
val sqlContext = new SapSQLContext (sc)

sqlContext.sql("show tables").show

==============
== IN SHELL ==
==============

clear
-- hdfs dfs -rm /user/vora/hanadata.csv
hdfs dfs -ls /user/vora
echo "1,1,2015-10-12,3" > /home/vora/hanadata.csv
hdfs dfs -put /home/vora/hanadata.csv
hdfs dfs -cat /user/vora/hanadata.csv

============== 
== IN SCALA ==
============== 

import java.text.SimpleDateFormat
import java.sql.Date
val simpleDateFormat: SimpleDateFormat = new SimpleDateFormat("yyyy-mm-dd")

val HANA_TABLE_PREFIX = "HANA_"
val HANA_HOSTNAME = "??.??.??.??"
val HANA_SCHEMA = "SHAUSER"
val HANA_USERNAME = "SHAUSER"
val HANA_PASSWORD = "SHALive1"
val HANA_INSTANCE = "00"

-- back to Spark Shell
-- make sure table is deleted from HANA
sqlContext.sql( s"""
    CREATE TEMPORARY TABLE ${HANA_TABLE_PREFIX}DATA (
      userid INTEGER,
      productid INTEGER,
      orderdate DATE,
      quantity INTEGER
) USING
      com.sap.spark.hana
    OPTIONS (
      path         "${HANA_TABLE_PREFIX}DATA",
      host         "${HANA_HOSTNAME}",
      dbschema     "${HANA_SCHEMA}",
      user         "${HANA_USERNAME}",
      passwd       "${HANA_PASSWORD}",
      instance     "${HANA_INSTANCE}"
    )
    """.stripMargin )

sqlContext.sql("show tables").show
-- but don't run a select * as data not loaded

import scala.collection.mutable.Map

val HANA_LOAD_OPTIONS = Map(
  "path"      ->  "",
  "host"      -> s"${HANA_HOSTNAME}",
  "Instance"  -> s"${HANA_INSTANCE}",
  "dbschema"  -> s"${HANA_SCHEMA}",
  "user"      -> s"${HANA_USERNAME}",
  "passwd"    -> s"${HANA_PASSWORD}",
  "instance"  -> s"${HANA_INSTANCE}"
)

-- use if table(s) exist on HANA. 
-- Will get an error from hana trying to drop a table that does not exist (no such table exception)
-- sqlContext.sql(s"""drop table ${HANA_SCHEMA}.${HANA_TABLE_PREFIX}DATA""")

    case class Hana_Data(
      userid: Integer,
      productid: Integer,
      orderdate: Date,
      quantity: Integer
    )
    
    
-------------------------------------------------------
    alternative script
    
    val data = Array(1, 2)
    val hana_dataText = sc.parallelize(data)
    
    val hana_datardd = hana_dataText.map( s => Hana_Data(
        s,
        s + 1
    )).toDF
-------------------------------------------------------    

    val hana_dataText = sc.textFile( s"hdfs://${HDFS_NAMENODE}${TPCH_DATA_PATH}hanadata.csv" )

    val hana_datardd = hana_dataText.map(s => s.split(",")).map( s => Hana_Data(
                s(0).toInt,
                s(1).toInt,
                new Date(simpleDateFormat.parse(s(2)).getTime()),
                s(3).toInt
         )
).toDF

HANA_LOAD_OPTIONS("path") = s"${HANA_TABLE_PREFIX}DATA"

sqlContext.sql("show tables").show

sqlContext.sql("SELECT * FROM HANA_DATA").show
-- will get error, but show error ; no table in hana

-- show hana, should be no table
hana_datardd.write.format("com.sap.spark.hana").mode(SaveMode.Overwrite).options(HANA_LOAD_OPTIONS).save()
-- show hana, should be a table now

sqlContext.sql("show tables").show

sqlContext.sql("SELECT * FROM HANA_DATA").show
